This project, titled Persian Food Segmentation using Deep Learning, presents a complete semantic segmentation framework designed to identify and separate different components of Persian food images before and after consumption. Developed by Shayan Rokhva (2025), the code integrates state-of-the-art deep learning techniques based on convolutional neural networks (CNNs), particularly U-Net and U-Net++, to perform precise pixel-level segmentation of complex and heterogeneous food compositions. The main objective is to create a robust model capable of analyzing Persian food dishes captured in real-world conditions and to estimate the portion of food consumed or remaining, providing a foundation for applications such as automated dietary monitoring, food waste estimation, and smart canteen analytics.

The dataset used in this project comprises thousands of images categorized into ten distinct Persian food classes, each containing both images and corresponding masks. Every image is stored in .jpg format, while masks use the .png format with the same filename, differing only by the “mask” suffix. Each class represents a specific type of meal, for example, Adas Polo, Fesenjan, Chelo Goosht, or Protein and Fries, where each dish has multiple semantic labels like background, stew, rice, or protein. The dataset is split into “before” and “after” subsets to represent the food’s state before and after consumption. Several preprocessing operations ensure the correctness and consistency of these pairs. The notebook verifies the correspondence between each image and its mask, unifies and reorganizes datasets, and generates histograms showing the pixel-level distribution of all semantic classes. It also includes automated merging of “before” and “after” folders for each food type and can dynamically select a single category for focused model training.

To prepare the data for deep learning, the notebook defines a custom SegmentationDataset class that automatically loads and preprocesses images and masks. Each image is converted to a PyTorch tensor and normalized, while the mask is converted to grayscale and encoded into numerical class labels according to predefined mappings. Optional transformations such as resizing, horizontal flipping, or random brightness changes can be applied using Albumentations or Torchvision transforms to augment the dataset and improve generalization. This design allows efficient mini-batch loading through DataLoader, enabling large-scale training on GPUs while maintaining a clean and reproducible workflow.

The model section of the notebook provides two main architectures: U-Net and U-Net++, which can be switched easily depending on experimental needs. The standard U-Net architecture is composed of an encoder-decoder structure with skip connections that transmit spatial information directly between downsampling and upsampling paths. The encoder progressively extracts features through convolutional and pooling operations, while the decoder reconstructs high-resolution segmentation maps via up-convolutions. Padding is enabled to maintain consistent dimensions between input and output. The U-Net++ variant enhances this design by introducing nested and dense skip connections, which better align semantic information across encoder and decoder stages and minimize the semantic gap between feature maps of different resolutions. This design refinement allows U-Net++ to achieve higher accuracy on complex and irregular boundaries typical of Persian dishes, where textures and colors vary substantially within a single plate.

A key innovation in this notebook is the implementation of a Capped Dynamic Weighted Cross-Entropy Loss function. Standard cross-entropy loss often struggles with highly imbalanced datasets, where background pixels dominate and small classes (like fries or stew) are underrepresented. To address this, the custom loss automatically computes inverse-frequency weights for each class within every training batch, emphasizing underrepresented classes while down-weighting the background. However, to avoid instability caused by extreme ratios, a cap is introduced—typically limiting the maximum weight ratio to 10×. This mechanism ensures a stable yet adaptive weighting scheme, preventing gradient explosions while maintaining fairness across all categories. This dynamic loss function has proven particularly effective for semantic segmentation tasks with significant imbalance, improving both convergence stability and per-class accuracy.

The training loop is built around PyTorch’s standard training workflow, enhanced for clarity and robustness. It automatically detects whether CUDA is available and uses the GPU if possible. During each epoch, the model performs forward and backward propagation, calculates the capped dynamic weighted loss, and updates the parameters using the Adam optimizer or Stochastic Gradient Descent. For evaluation, several metrics are calculated, including pixel accuracy, Intersection over Union (IoU), F1-score (Dice coefficient), and both macro and weighted averages. Weighted metrics are emphasized since they provide a more realistic reflection of performance under class imbalance. The notebook records and visualizes the evolution of training and validation loss curves and performance metrics using Matplotlib, following a clean academic plotting style with serif fonts and minimal clutter. Checkpoints are periodically saved, ensuring that the best-performing model parameters are preserved.

After training, the notebook includes a comprehensive post-processing and visualization stage. Predictions are overlaid on original images to qualitatively assess segmentation quality, while histograms illustrate pixel distributions of predicted versus true masks. The results demonstrate that the trained model successfully distinguishes between multiple food components with high precision. In particular, U-Net++ achieves sharper segmentation boundaries and improved delineation of small or overlapping food regions. Comparative plots between “before” and “after” consumption images provide visual evidence for estimating consumption ratios—information that can be used to compute eating rate or remaining portions without physical measurement. All visualization results are annotated with class labels such as “Rice_Fesenjan” or “Fries,” making interpretation intuitive and directly relevant to food consumption analysis.

The code is organized in a clear and modular structure. It starts with dataset verification and histogram generation, followed by dataset definition, model architecture, custom loss function, training loop, and finally visualization and interpretation. This structure allows easy adaptation and reusability for other segmentation tasks beyond food recognition. Moreover, the modularity enables researchers to experiment with different backbones, optimizers, or learning rates by modifying only specific sections of the code.

Overall, this project provides a complete, research-oriented framework for deep learning–based semantic segmentation in food imagery. By combining strong architectural baselines with a custom loss function and precise preprocessing, it achieves robust segmentation results despite noisy lighting conditions, varied textures, and significant intra-class diversity. Beyond its technical achievement, the notebook demonstrates how computer vision can be applied to culturally specific datasets, offering valuable insights into Persian cuisine while contributing to global efforts in food waste monitoring and sustainable consumption. The Persian Food Segmentation framework thus stands as both a practical implementation and a scientifically valuable contribution, bridging the gap between AI research and real-world applications in the food and nutrition domain.
